# lecture_streamlit.py
# Streamlit + LangChain ê¸°ë°˜ Lecture-RAG ì›¹ì•±
# ================================================
# 
# [ê¸°ìˆ  ìŠ¤íƒ]
# - Frontend: Streamlit (ì›¹ UI í”„ë ˆì„ì›Œí¬)
# - LLM: OpenAI GPT (langchain-openai)
# - Vector DB: FAISS (Facebook AI Similarity Search)
# - Embeddings: HuggingFace sentence-transformers (all-MiniLM-L6-v2)
# - Text Processing: LangChain RecursiveCharacterTextSplitter
# - Language: Python 3.8+
# 
# [ì£¼ìš” ê¸°ëŠ¥]
# - ê°•ì˜ë¡ ë¬¸ì„œ ì¸ë±ì‹± (ë²¡í„° ì„ë² ë”© ìƒì„± ë° ì €ì¥)
# - ì˜ë¯¸ì  ìœ ì‚¬ë„ ê²€ìƒ‰ (ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œ ì¡°ê° ê²€ìƒ‰)
# - ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ LLM ë‹µë³€ ìƒì„±
# - í—ˆìš©ëœ ëª¨ë“ˆ/ì‹¬ë³¼ë§Œ ì‚¬ìš©í•˜ë„ë¡ ì½”ë“œ ìƒì„± ì œí•œ
# - ë¯¸í—ˆìš© í† í° ê°ì§€ ì‹œ ìë™ ì¬ìƒì„±
# - ê·¼ê±° ë¬¸ì„œ ì¡°ê° ì œê³µ (explainability)
# 
# [ì„¤ì¹˜ ë° ì‹¤í–‰]
# ì˜ì¡´ì„± ì„¤ì¹˜:
#   pip install -U streamlit "langchain>=0.2" langchain-community faiss-cpu sentence-transformers langchain-openai
# ì‹¤í–‰:
#   streamlit run lecture_streamlit.py
# í™˜ê²½ë³€ìˆ˜:
#   export OPENAI_API_KEY=your_api_key_here  # í•„ìˆ˜
#   export LECTURE_RAG_MODEL=gpt-4o-mini     # ì„ íƒ (ê¸°ë³¸ê°’)
#   export LECTURE_RAG_TEMPERATURE=0.2       # ì„ íƒ (ê¸°ë³¸ê°’)
# 
# [ì‹œìŠ¤í…œ í”„ë¡œì„¸ìŠ¤ í”Œë¡œìš°]
# 1. ì¸ë±ì‹± ë‹¨ê³„:
#    ê°•ì˜ë¡ íŒŒì¼ â†’ í…ìŠ¤íŠ¸/ì½”ë“œ ë¸”ë¡ ë¶„ë¦¬ â†’ ì²­í‚¹ â†’ ì„ë² ë”© ìƒì„± â†’ FAISS ì €ì¥
#    í—ˆìš© í† í°(ëª¨ë“ˆ/ì‹¬ë³¼) ì¶”ì¶œ â†’ JSON ì €ì¥
# 
# 2. ì§ˆì˜ì‘ë‹µ ë‹¨ê³„:
#    ì‚¬ìš©ì ì§ˆë¬¸ â†’ ì„ë² ë”© ë³€í™˜ â†’ FAISS ìœ ì‚¬ë„ ê²€ìƒ‰ â†’ ìƒìœ„ Kê°œ ë¬¸ì„œ ì¡°ê° ê²€ìƒ‰
#    â†’ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± â†’ LLM í”„ë¡¬í”„íŠ¸ ìƒì„± â†’ GPT ë‹µë³€ ìƒì„±
#    â†’ ìƒì„±ëœ ì½”ë“œì—ì„œ ë¯¸í—ˆìš© í† í° ê²€ì‚¬ â†’ (í•„ìš”ì‹œ) ì¬ìƒì„± â†’ ìµœì¢… ë‹µë³€ ë°˜í™˜
# 
# 3. ë³´ì•ˆ ê²€ì¦:
#    ìƒì„±ëœ ì½”ë“œ â†’ ì •ê·œì‹ìœ¼ë¡œ import/í•¨ìˆ˜ ì¶”ì¶œ â†’ í—ˆìš© í† í°ê³¼ ë¹„êµ
#    â†’ ë¯¸í—ˆìš© í† í° ë°œê²¬ì‹œ ì œì•½ì¡°ê±´ ì¶”ê°€í•˜ì—¬ ì¬ìš”ì²­

from __future__ import annotations
import os
import re
import json
from typing import List, Tuple, Dict, Any, Optional
from pathlib import Path

import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI

# ---------------------------
# Utilities (ì›ë³¸ ë¡œì§ ìµœëŒ€ ìœ ì§€)
# ---------------------------

def read_text(path: Path) -> str:
    return path.read_text(encoding="utf-8", errors="ignore")

def detect_code_blocks(text: str) -> List[Tuple[str, str]]:
    """``` fenced code ``` ìš°ì„ , ì—†ìœ¼ë©´ ê°„ì´ ì½”ë“œ ê°ì§€."""
    blocks: List[Tuple[str, str]] = []
    fence_pattern = re.compile(r"```(?:[a-zA-Z0-9_+-]+)?\s*\n(.*?)```", re.DOTALL)
    pos = 0

    # ë¨¼ì € fencedë¥¼ ì „ë¶€ ì°¾ì•„ì„œ code ì²˜ë¦¬í•˜ê³  ì‚¬ì´ ì‚¬ì´ë¥¼ textë¡œ
    matches = list(fence_pattern.finditer(text))
    if matches:
        for m in matches:
            before = text[pos:m.start()].strip()
            if before:
                blocks.append(("text", before))
            code = m.group(1)
            blocks.append(("code", code))
            pos = m.end()
        tail = text[pos:].strip()
        if tail:
            blocks.append(("text", tail))
        return blocks

    # fallback: ê°„ì´ ì½”ë“œ ê°ì§€
    lines = text.splitlines()
    buf: List[str] = []
    mode = "text"

    def flush():
        nonlocal buf, mode
        if buf:
            blocks.append((mode, "\n".join(buf).strip()))
            buf = []

    codey = re.compile(r"^(\s*(def |class |import |from |if __name__ == '__main__':|for |while |try:|except |with ))")
    for ln in lines:
        if codey.search(ln):
            if mode != "code":
                flush(); mode = "code"
            buf.append(ln)
        else:
            if mode != "text":
                flush(); mode = "text"
            buf.append(ln)
    flush()
    return blocks

def chunk_documents(text: str, source: str) -> List[Document]:
    blocks = detect_code_blocks(text)
    docs: List[Document] = []
    
    # ì „ì²´ í…ìŠ¤íŠ¸ì˜ ë¼ì¸ ë²ˆí˜¸ ë§¤í•‘ì„ ìœ„í•œ ì¤€ë¹„
    all_lines = text.splitlines()
    total_lines = len(all_lines)

    code_splitter = RecursiveCharacterTextSplitter(
        chunk_size=600, chunk_overlap=80, separators=["\n\n", "\n", ")\n", ":\n", ",\n"]
    )
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800, chunk_overlap=120, separators=["\n\n", "\n", ". "]
    )
    
    idx = 0
    current_line = 1
    
    for kind, content in blocks:
        block_lines = content.splitlines()
        block_start_line = current_line
        
        splitter = code_splitter if kind == "code" else text_splitter
        chunks = splitter.split_text(content)
        
        for chunk in chunks:
            chunk_lines = chunk.splitlines()
            chunk_start_line = current_line
            chunk_end_line = current_line + len(chunk_lines) - 1
            
            # ì²­í¬ì˜ ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ì¤„ì˜ ì¼ë¶€ ë‚´ìš©ì„ ë¯¸ë¦¬ë³´ê¸°ë¡œ ì €ì¥
            first_line_preview = chunk_lines[0][:50] + "..." if len(chunk_lines[0]) > 50 else chunk_lines[0]
            last_line_preview = chunk_lines[-1][:50] + "..." if len(chunk_lines[-1]) > 50 else chunk_lines[-1]
            
            docs.append(
                Document(
                    page_content=chunk,
                    metadata={
                        "source": source, 
                        "kind": kind, 
                        "chunk_id": f"{source}:{idx}",
                        "start_line": chunk_start_line,
                        "end_line": chunk_end_line,
                        "total_lines": total_lines,
                        "line_count": len(chunk_lines),
                        "first_line_preview": first_line_preview,
                        "last_line_preview": last_line_preview
                    },
                )
            )
            current_line += len(chunk_lines)
            idx += 1
            
        # ë¸”ë¡ ê°„ ì¤„ë°”ê¿ˆ ê³ ë ¤
        if current_line <= total_lines:
            current_line += 1
            
    return docs

def extract_allowed_tokens(text: str) -> Dict[str, Any]:
    imports = re.findall(r"^(?:from\s+([\w\.]+)\s+import\s+([\w\*,\s]+)|import\s+([\w\.,\s]+))", text, re.M)
    modules = set()
    names = set()
    for a, b, c in imports:
        if a:
            modules.add(a)
            for nm in re.split(r"\s*,\s*", b.strip()):
                if nm:
                    names.add(nm)
        elif c:
            for m in re.split(r"\s*,\s*", c.strip()):
                if m:
                    modules.add(m)

    funcs = re.findall(r"^\s*def\s+([a-zA-Z_][\w]*)\(", text, re.M)
    klass = re.findall(r"^\s*class\s+([A-Z][\w]*)\(", text, re.M)
    consts = re.findall(r"^([A-Z_]{3,})\s*=\s*", text, re.M)

    return {
        "modules": sorted(modules),
        "symbols": sorted(set(funcs) | set(klass) | set(consts)),
    }

def find_unknown_tokens(code: str, allowed: Dict[str, Any]) -> List[str]:
    unknown = []
    for m in re.finditer(r"^(?:from\s+([\w\.]+)\s+import\s+([\w\*,\s]+)|import\s+([\w\.,\s]+))", code, re.M):
        mod_from, names, mods = m.groups()
        if mod_from and mod_from not in allowed["modules"]:
            unknown.append(f"module:{mod_from}")
        if names:
            for nm in re.split(r"\s*,\s*", names.strip()):
                nm = nm.strip()
                if nm and nm not in allowed["symbols"]:
                    unknown.append(f"symbol:{nm}")
        if mods:
            for md in re.split(r"\s*,\s*", mods.strip()):
                md = md.strip()
                if md and md not in allowed["modules"]:
                    unknown.append(f"module:{md}")
    return sorted(set(unknown))

# ---------------------------
# Indexing & Retrieval (with cache)
# ---------------------------

@st.cache_resource(show_spinner=False)
def get_embeddings():
    return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

def build_or_load_store(store_dir: Path):
    store_dir.mkdir(parents=True, exist_ok=True)
    return store_dir

def do_index(lecture_path: Path, store_dir: Path):
    text = read_text(lecture_path)
    docs = chunk_documents(text, source=str(lecture_path))

    embeddings = get_embeddings()
    vs = FAISS.from_documents(docs, embedding=embeddings)
    vs.save_local(str(store_dir))

    allowed = extract_allowed_tokens(text)
    (store_dir / "allowed_tokens.json").write_text(
        json.dumps(allowed, ensure_ascii=False, indent=2), encoding="utf-8"
    )
    return len(docs), allowed

@st.cache_resource(show_spinner=False)
def load_vectorstore(store_dir_str: str):
    embeddings = get_embeddings()
    return FAISS.load_local(store_dir_str, embeddings, allow_dangerous_deserialization=True)

def retrieve(query: str, store_dir: Path, k: int = 8) -> Tuple[List[Document], Dict[str, Any]]:
    vs = load_vectorstore(str(store_dir))
    docs = vs.similarity_search(query, k=k)
    allowed_path = store_dir / "allowed_tokens.json"
    allowed = {"modules": [], "symbols": []}
    if allowed_path.exists():
        allowed = json.loads(allowed_path.read_text(encoding="utf-8"))
    return docs, allowed

# ---------------------------
# LLM Prompting
# ---------------------------

SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ ìˆ˜ì—…ìš© ì½”ì¹˜ì…ë‹ˆë‹¤.\n"
    "ë°˜ë“œì‹œ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸(ê°•ì˜ë¡ ì¡°ê°ë“¤)ì—ì„œë§Œ ê·¼ê±°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µí•˜ì‹­ì‹œì˜¤.\n"
    "ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬, í”„ë ˆì„ì›Œí¬, í•¨ìˆ˜ ì‚¬ìš©ì€ ê¸ˆì§€í•©ë‹ˆë‹¤.\n"
    "ê·¼ê±°ê°€ ë¶€ì¡±í•˜ë©´ ì •ì¤‘íˆ ê±°ì ˆí•˜ê³ , ê°•ì˜ë¡ì—ì„œ ê°€ê¹Œìš´ ëŒ€ì•ˆ ì ˆì°¨ë¥¼ ì œì•ˆí•˜ì„¸ìš”.\n"
    "ê°€ëŠ¥í•˜ë©´ ê°•ì˜ë¡ì˜ ë³€ìˆ˜ëª…/í•¨ìˆ˜ëª…/ìŠ¤íƒ€ì¼ì„ ëª¨ë°©í•˜ì„¸ìš”.\n"
)
ANSWER_GUIDE = (
    "ì¶œë ¥ í˜•ì‹:\n"
    "1) ê°„ë‹¨í•œ ì„¤ëª…\n"
    "2) ì½”ë“œ(í•„ìš” ì‹œ)\n"
    "3) ì‚¬ìš©í•œ ê·¼ê±° ìŠ¤ë‹ˆí«ë“¤(ì‹ë³„ì/ìš”ì•½)\n"
    "ì£¼ì˜: ì½”ë“œ ë¸”ë¡ì€ ë°˜ë“œì‹œ ```python ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”."
)

def make_context_block(docs: List[Document]) -> str:
    parts = []
    for i, d in enumerate(docs, 1):
        meta = d.metadata
        location_info = f"ë¼ì¸ {meta.get('start_line', '?')}-{meta.get('end_line', '?')}"
        chunk_info = f"[Chunk {i} | {meta.get('kind')} | {location_info} | {meta.get('chunk_id')}]"
        parts.append(f"{chunk_info}\n{d.page_content}")
    return "\n\n".join(parts)

def call_llm(query: str, docs: List[Document], allowed: Dict[str, Any], extra_file_text: Optional[str] = None) -> str:
    temp = float(os.getenv("LECTURE_RAG_TEMPERATURE", "0.2"))
    model_name = os.getenv("LECTURE_RAG_MODEL", "gpt-4o-mini")
    llm = ChatOpenAI(model=model_name, temperature=temp)

    style_hint = (
        "í—ˆìš© ëª¨ë“ˆ: " + ", ".join(allowed.get("modules", [])) + "\n"
        "í—ˆìš© ì‹¬ë³¼: " + ", ".join(allowed.get("symbols", [])) + "\n"
    )

    context = make_context_block(docs)
    if extra_file_text:
        context = context + "\n\n[UserFile]\n" + extra_file_text

    messages = [
        ("system", SYSTEM_PROMPT),
        ("user", f"ì§ˆë¬¸: {query}\n\n{ANSWER_GUIDE}\n\n[ì»¨í…ìŠ¤íŠ¸ ì‹œì‘]\n{context}\n[ì»¨í…ìŠ¤íŠ¸ ë]\n\n{style_hint}\n")
    ]
    resp = llm.invoke(messages)
    return resp.content

# ---------------------------
# Streamlit UI
# ---------------------------

st.set_page_config(page_title="Lecture-RAG (Streamlit)", page_icon="ğŸ“š", layout="wide")
st.title("Lecture-RAG - ê°•ì˜ë¡ ìŠ¤íƒ€ì¼ ê°•ì œ RAG (Streamlit)")

with st.sidebar:
    st.header("ì„¤ì •")
    default_store = ".lecture_index"
    store_dir_str = st.text_input("FAISS ì €ì¥ ë””ë ‰í„°ë¦¬", value=default_store)
    store_dir = build_or_load_store(Path(store_dir_str))

    model = st.text_input("LLM ëª¨ë¸ (env LECTURE_RAG_MODEL ìš°ì„ )", value=os.getenv("LECTURE_RAG_MODEL", "gpt-4o-mini"))
    temp = st.slider("Temperature", 0.0, 1.0, float(os.getenv("LECTURE_RAG_TEMPERATURE", "0.2")), 0.05)
    st.caption("â€» OpenAI ì‚¬ìš© ì‹œ í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    st.divider()
    st.subheader("ì¸ë±ì‹±")
    upload = st.file_uploader("ê°•ì˜ë¡ íŒŒì¼ ì—…ë¡œë“œ(.txt, .md ë“±)", type=["txt", "md", "py", "mdx"], accept_multiple_files=False)
    manual_path = st.text_input("ë˜ëŠ” ë¡œì»¬ ê°•ì˜ë¡ ê²½ë¡œ ì…ë ¥", value="ê°•ì˜ë¡.txt")

    if st.button("ì¸ë±ì‹± ì‹¤í–‰", type="primary"):
        if upload is not None:
            tmp_path = Path(store_dir) / "uploaded_lecture.txt"
            tmp_path.write_bytes(upload.read())
            path_to_index = tmp_path
        else:
            path_to_index = Path(manual_path)

        if not path_to_index.exists():
            st.error(f"ê°•ì˜ë¡ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path_to_index}")
        else:
            # í™˜ê²½ ê°’ ì£¼ì…(ì„ íƒ)
            os.environ["LECTURE_RAG_MODEL"] = model
            os.environ["LECTURE_RAG_TEMPERATURE"] = str(temp)
            with st.spinner("ì¸ë±ì‹± ì¤‘..."):
                n_docs, allowed = do_index(path_to_index, store_dir)
            st.success(f"ì¸ë±ì‹± ì™„ë£Œ! ë¬¸ì„œ ì¡°ê° {n_docs}ê°œ ìƒì„±")
            with st.expander("í—ˆìš© í† í°(ëª¨ë“ˆ/ì‹¬ë³¼) ë³´ê¸°"):
                st.json(allowed)

st.divider()
colQ, colOpt = st.columns([2, 1], vertical_alignment="top")
with colQ:
    st.subheader("ì§ˆì˜ì‘ë‹µ")
    query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ) ë¦¬ìŠ¤íŠ¸ë¥¼ ì—­ìˆœìœ¼ë¡œ ì •ë ¬í•˜ëŠ” í•¨ìˆ˜ ë§Œë“¤ì–´ì¤˜")
with colOpt:
    topk = st.slider("Top-K ë¬¸ì„œ", min_value=2, max_value=15, value=8, step=1)
    ref_file = st.file_uploader("ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ë¡œ ë‚´ ì½”ë“œ íŒŒì¼ ì²¨ë¶€(ì„ íƒ)", type=["py","txt","md"])

ask = st.button("ë‹µë³€ ìƒì„±", type="primary")

if ask:
    if not query.strip():
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        # í™˜ê²½ ê°’ ì£¼ì…(ì„ íƒ)
        os.environ["LECTURE_RAG_MODEL"] = model
        os.environ["LECTURE_RAG_TEMPERATURE"] = str(temp)

        # ê²€ìƒ‰
        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
            try:
                docs, allowed = retrieve(query, store_dir, k=topk)
            except Exception as e:
                st.error(f"ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì¸ë±ì‹±ì„ ìˆ˜í–‰í•˜ì„¸ìš”. ìƒì„¸: {e}")
                docs, allowed = [], {"modules": [], "symbols": []}

        if not docs:
            st.error("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì¸ë±ì‹±ì„ ë¨¼ì € ìˆ˜í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        else:
            extra_text = None
            if ref_file is not None:
                try:
                    extra_text = ref_file.read().decode("utf-8", errors="ignore")
                except Exception:
                    extra_text = None

            # 1ì°¨ ìƒì„±
            with st.spinner("LLM ì‘ë‹µ ìƒì„± ì¤‘..."):
                answer = call_llm(query, docs, allowed, extra_file_text=extra_text)

            # ë¯¸í—ˆìš© ì‹¬ë³¼ ê²€ì‚¬ â†’ 1íšŒ ì¬ì‹œë„
            code_blocks = re.findall(r"```(?:python)?\n(.*?)```", answer, re.DOTALL)
            unknown_total: List[str] = []
            for cb in code_blocks:
                unknown_total.extend(find_unknown_tokens(cb, allowed))

            if unknown_total:
                with st.expander("ë¯¸í—ˆìš© í† í° ê°ì§€ (ìë™ ì¬ìƒì„± ì „ ë¡œê·¸)", expanded=False):
                    st.write(", ".join(sorted(set(unknown_total))))
                retry_prompt = query + "\n(ì£¼ì˜: ì•„ë˜ ë¯¸í—ˆìš© ëª©ë¡ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”: " + ", ".join(sorted(set(unknown_total))) + ")"
                with st.spinner("ë¯¸í—ˆìš© í† í° ì œê±°í•˜ì—¬ ì¬ìƒì„± ì¤‘..."):
                    answer = call_llm(retry_prompt, docs, allowed, extra_file_text=extra_text)

            st.subheader("ë‹µë³€")
            st.markdown(answer)

            st.subheader("ê·¼ê±° ìŠ¤ë‹ˆí«")
            for i, d in enumerate(docs, 1):
                meta = d.metadata
                location_info = f"ë¼ì¸ {meta.get('start_line', '?')}-{meta.get('end_line', '?')}"
                first_line = meta.get('first_line_preview', '')
                
                # ë” ìì„¸í•œ ì œëª© í‘œì‹œ
                title = f"Chunk {i} | {meta.get('kind')} | {location_info} | ì‹œì‘: {first_line}"
                
                with st.expander(title, expanded=False):
                    # ìœ„ì¹˜ ì •ë³´ í‘œì‹œ
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.caption(f"ğŸ“ ìœ„ì¹˜: {location_info}")
                    with col2:
                        st.caption(f"ğŸ“ ì¤„ ìˆ˜: {meta.get('line_count', '?')}ì¤„")
                    with col3:
                        st.caption(f"ğŸ·ï¸ íƒ€ì…: {meta.get('kind')}")
                    
                    # ë‚´ìš© í‘œì‹œ
                    snippet = d.page_content
                    language = "python" if meta.get('kind') == 'code' else "text"
                    st.code(snippet, language=language)
                    
                    # ì›ë³¸ íŒŒì¼ì—ì„œ ì°¾ëŠ” ë°©ë²• ì•ˆë‚´
                    st.info(f"ğŸ’¡ ì›ë³¸ì—ì„œ ì°¾ê¸°: '{meta.get('first_line_preview', '')}' ê²€ìƒ‰í•˜ì—¬ {location_info} í™•ì¸")
